# Overall results

| dataset             | task             | approach   |   Llama-3.2-3B precision |   Llama-3.2-3B recall |   Llama-3.2-3B f1-score |   EuroLLM-9B precision |   EuroLLM-9B recall |   EuroLLM-9B f1-score |   Teuken-7B precision |   Teuken-7B recall |   Teuken-7B f1-score |   BübleLM precision |   BübleLM recall |   BübleLM f1-score |
|:--------------------|:-----------------|:-----------|-------------------------:|----------------------:|------------------------:|-----------------------:|--------------------:|----------------------:|----------------------:|-------------------:|---------------------:|--------------------:|-----------------:|-------------------:|
| covid19             | informativeness  | zs         |                 0.838542 |              0.612808 |               0.590435  |              0.519048  |           0.338807  |             0.34519   |             0.673893  |           0.533333 |            0.491977  |                     |                  |                    |
| covid19             |                  | fs         |                 0.500978 |              0.565298 |               0.517766  |              0.748605  |           0.57121   |             0.628054  |             0.680037  |           0.634154 |            0.653288  |                     |                  |                    |
| covid19             |                  | t          |                 0.702469 |              0.727531 |               0.70028   |              0.769749  |           0.792228  |             0.779943  |             0.647741  |           0.677942 |            0.647627  |            0.728136 |         0.7237   |           0.722761 |
| covid19             | topic            | zs         |                 0.233333 |              0.229532 |               0.175309  |              0.305958  |           0.169514  |             0.198914  |             0.460917  |           0.37115  |            0.294857  |                     |                  |                    |
| covid19             |                  | fs         |                 0.50591  |              0.442207 |               0.39872   |              0.239926  |           0.241133  |             0.165348  |             0.620837  |           0.376286 |            0.31353   |                     |                  |                    |
| covid19             |                  | t          |                 0.711156 |              0.656191 |               0.671934  |              0.515     |           0.532677  |             0.502289  |             0.541453  |           0.614297 |            0.560508  |            0.609117 |         0.571011 |           0.575528 |
| covid19             | credibility      | zs         |                 0.332515 |              0.172543 |               0.0970674 |              0.466667  |           0.0352564 |             0.0653179 |             0.411425  |           0.388889 |            0.381033  |                     |                  |                    |
| covid19             |                  | fs         |                 0.537415 |              0.357906 |               0.412675  |              0.484538  |           0.494124  |             0.36923   |             0.557854  |           0.50641  |            0.443985  |                     |                  |                    |
| covid19             |                  | t          |                 0.49686  |              0.518697 |               0.503779  |              0.541085  |           0.564637  |             0.541822  |             0.46875   |           0.48344  |            0.469064  |            0.506623 |         0.517628 |           0.51011  |
| speech_acts_coarse  | coarse labels    | zs         |                 0.296795 |              0.221922 |               0.142433  |              0.192914  |           0.224964  |             0.170898  |             0.253561  |           0.184994 |            0.115082  |                     |                  |                    |
| speech_acts_coarse  |                  | fs         |                 0.418397 |              0.321895 |               0.287129  |              0.208346  |           0.205111  |             0.174202  |             0.187965  |           0.237892 |            0.191404  |                     |                  |                    |
| speech_acts_coarse  |                  | t          |                 0.642365 |              0.6705   |               0.651282  |              0.690719  |           0.562598  |             0.587731  |             0.579138  |           0.602185 |            0.560238  |            0.479687 |         0.5356   |           0.48655  |
| speech_acts_fine    | fine labels      | zs         |                 0.134167 |              0.11147  |               0.100505  |              0.156891  |           0.126369  |             0.116417  |             0.0338756 |           0.106085 |            0.0308676 |                     |                  |                    |
| speech_acts_fine    |                  | fs         |                 0.277995 |              0.200424 |               0.171971  |              0.0980564 |           0.116942  |             0.0688366 |             0.106574  |           0.169794 |            0.095387  |                     |                  |                    |
| speech_acts_fine    |                  | t          |                 0.326283 |              0.377671 |               0.338538  |              0.33945   |           0.321864  |             0.311157  |             0.39955   |           0.425345 |            0.389196  |            0.274877 |         0.312568 |           0.282974 |
| hasoc2020           | coarse labels    | zs         |                 0.660604 |              0.619175 |               0.59467   |              0.598585  |           0.354497  |             0.374718  |             0.628846  |           0.507653 |            0.219968  |                     |                  |                    |
| hasoc2020           |                  | fs         |                 0.699169 |              0.491776 |               0.552692  |              0.633367  |           0.609351  |             0.598656  |             0.630604  |           0.516582 |            0.239209  |                     |                  |                    |
| hasoc2020           |                  | t          |                 0.763578 |              0.809968 |               0.778599  |              0.745536  |           0.791121  |             0.759394  |             0.776941  |           0.805437 |            0.788603  |            0.784705 |         0.784705 |           0.784705 |
| hasoc2020           | fine labels      | zs         |                 0.378451 |              0.37751  |               0.285505  |              0.320071  |           0.264144  |             0.16715   |             0.259561  |           0.268519 |            0.0682963 |                     |                  |                    |
| hasoc2020           |                  | fs         |                 0.391596 |              0.303541 |               0.294458  |              0.368866  |           0.304278  |             0.23011   |             0.229322  |           0.284392 |            0.252714  |                     |                  |                    |
| hasoc2020           |                  | t          |                 0.491798 |              0.584912 |               0.502287  |              0.48436   |           0.558487  |             0.506802  |             0.462343  |           0.58918  |            0.484178  |            0.489711 |         0.538976 |           0.505654 |
| germeval2019_task12 | coarse labels    | zs         |                 0.661932 |              0.621493 |               0.560364  |              0.65009   |           0.600811  |             0.544879  |             0.160013  |           0.5      |            0.242439  |                     |                  |                    |
| germeval2019_task12 |                  | fs         |                 0.651538 |              0.499429 |               0.33963   |              0.673643  |           0.669198  |             0.64088   |             0.160013  |           0.5      |            0.242439  |                     |                  |                    |
| germeval2019_task12 |                  | t          |                 0.760854 |              0.767516 |               0.763916  |              0.756021  |           0.777762  |             0.763213  |             0.731868  |           0.755838 |            0.738039  |            0.734029 |         0.763871 |           0.738084 |
| germeval2019_task12 | fine labels      | zs         |                 0.363981 |              0.367897 |               0.356866  |              0.336127  |           0.301235  |             0.216257  |             0.29213   |           0.317321 |            0.206944  |                     |                  |                    |
| germeval2019_task12 |                  | fs         |                 0.408201 |              0.26014  |               0.278312  |              0.332255  |           0.291256  |             0.226164  |             0.401785  |           0.292394 |            0.113368  |                     |                  |                    |
| germeval2019_task12 |                  | t          |                 0.423738 |              0.453068 |               0.423495  |              0.39717   |           0.44684   |             0.403367  |             0.436117  |           0.478363 |            0.439623  |            0.404853 |         0.463122 |           0.413066 |
| germeval2019_task3  | offensive labels | zs         |                 0.535016 |              0.533076 |               0.260397  |              0.501231  |           0.391435  |             0.262656  |             0.427957  |           0.5      |            0.461182  |                     |                  |                    |
| germeval2019_task3  |                  | fs         |                 0.573144 |              0.508794 |               0.144903  |              0.533797  |           0.487203  |             0.357176  |             0.595238  |           0.50495  |            0.474751  |                     |                  |                    |
| germeval2019_task3  |                  | t          |                 0.675612 |              0.760369 |               0.69906   |              0.683015  |           0.707174  |             0.693609  |             0.671447  |           0.738056 |            0.692673  |            0.653832 |         0.733546 |           0.673448 |
| germeval2021        | toxic            | zs         |                 0.604357 |              0.600298 |               0.600887  |              0.56586   |           0.480808  |             0.39625   |             0.61499   |           0.531679 |            0.364985  |                     |                  |                    |
| germeval2021        |                  | fs         |                 0.591849 |              0.598389 |               0.586313  |              0.558338  |           0.486989  |             0.516932  |             0.62038   |           0.562848 |            0.436605  |                     |                  |                    |
| germeval2021        |                  | t          |                 0.674422 |              0.678687 |               0.676104  |              0.697029  |           0.688071  |             0.691462  |             0.701477  |           0.705012 |            0.703014  |            0.677823 |         0.663454 |           0.667712 |
| germeval2021        | engaging         | zs         |                 0.547461 |              0.543707 |               0.530494  |              0.558139  |           0.492092  |             0.463812  |             0.556836  |           0.522897 |            0.327142  |                     |                  |                    |
| germeval2021        |                  | fs         |                 0.510722 |              0.412091 |               0.429085  |              0.531076  |           0.423963  |             0.469485  |             0.586691  |           0.549032 |            0.352616  |                     |                  |                    |
| germeval2021        |                  | t          |                 0.661298 |              0.684232 |               0.668     |              0.662149  |           0.678552  |             0.668114  |             0.638046  |           0.659544 |            0.643219  |            0.667417 |         0.679859 |           0.672466 |
| germeval2021        | factClaiming     | zs         |                 0.625586 |              0.610889 |               0.528285  |              0.57138   |           0.479916  |             0.34391   |             0.666667  |           0.501582 |            0.25611   |                     |                  |                    |
| germeval2021        |                  | fs         |                 0.605958 |              0.61931  |               0.593268  |              0.584566  |           0.499019  |             0.538398  |             0.597159  |           0.547523 |            0.389737  |                     |                  |                    |
| germeval2021        |                  | t          |                 0.771303 |              0.767748 |               0.769452  |              0.752389  |           0.760641  |             0.755994  |             0.72104   |           0.722435 |            0.721721  |            0.715776 |         0.713669 |           0.714685 |

# Results per Class


# Zero-Shot Classification
## Llama-3.2-3B
### Covid19 Dataset
#### Task Informativeness
|                     |   precision |   recall |   f1-score |   support |
|:--------------------|------------:|---------:|-----------:|----------:|
| Informative         |    0.984375 | 0.724138 |   0.834437 |  87       |
| Personal_Experience |    1        | 0.142857 |   0.25     |   7       |
| none                |    0.53125  | 0.971429 |   0.686869 |  35       |
| accuracy            |    0.75969  | 0.75969  |   0.75969  |   0.75969 |
| macro avg           |    0.838542 | 0.612808 |   0.590435 | 129       |
| weighted avg        |    0.862282 | 0.75969  |   0.762686 | 129       |
#### Task Topic
|                   |   precision |   recall |   f1-score |    support |
|:------------------|------------:|---------:|-----------:|-----------:|
| Case_Report       |    0        | 0        |   0        |  37        |
| Consequences      |    0.25     | 0.166667 |   0.2      |   6        |
| Governm_Decisions |    0        | 0        |   0        |  22        |
| Risk_Reduction    |    0        | 0        |   0        |   3        |
| Vaccination       |    0.8      | 0.210526 |   0.333333 |  19        |
| none              |    0.35     | 1        |   0.518519 |  42        |
| accuracy          |    0.364341 | 0.364341 |   0.364341 |   0.364341 |
| macro avg         |    0.233333 | 0.229532 |   0.175309 | 129        |
| weighted avg      |    0.243411 | 0.364341 |   0.227218 | 129        |
#### Task Credibility
|              |   precision |    recall |   f1-score |   support |
|:-------------|------------:|----------:|-----------:|----------:|
| credible     |   0.75      | 0.0384615 |  0.0731707 |        78 |
| non-credible |   0.0217391 | 0.333333  |  0.0408163 |         3 |
| none         |   0.225806  | 0.145833  |  0.177215  |        48 |
| micro avg    |   0.135802  | 0.0852713 |  0.104762  |       129 |
| macro avg    |   0.332515  | 0.172543  |  0.0970674 |       129 |
| weighted avg |   0.538015  | 0.0852713 |  0.111133  |       129 |
### Speech_acts_coarse Dataset
#### Task Coarse labels
|              |   precision |    recall |   f1-score |   support |
|:-------------|------------:|----------:|-----------:|----------:|
| ASSERTIVE    |   0.785714  | 0.160584  |  0.266667  |       137 |
| COMMISSIVE   |   0         | 0         |  0         |         4 |
| DIRECTIVE    |   0.379845  | 0.385827  |  0.382812  |       127 |
| EXPRESSIVE   |   0.5       | 0.0375    |  0.0697674 |        80 |
| OTHER        |   0.0485437 | 0.714286  |  0.0909091 |        14 |
| UNSURE       |   0.0666667 | 0.0333333 |  0.0444444 |        30 |
| micro avg    |   0.221354  | 0.216837  |  0.219072  |       392 |
| macro avg    |   0.296795  | 0.221922  |  0.142433  |       392 |
| weighted avg |   0.506538  | 0.216837  |  0.238107  |       392 |
### Speech_acts_fine Dataset
#### Task Fine labels
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| ADDRESS      |    1        | 0.48     |   0.648649 |  75        |
| AGREE        |    0        | 0        |   0        |   3        |
| ASSERT       |    0.619048 | 0.110169 |   0.18705  | 118        |
| COMMISSIVE   |    0        | 0        |   0        |   4        |
| COMPLAIN     |    0.284404 | 0.607843 |   0.3875   |  51        |
| EXCLUDED     |    0        | 0        |   0        |   3        |
| GUESS        |    0        | 0        |   0        |   5        |
| OTHER        |    0.217391 | 0.333333 |   0.263158 |  15        |
| PREDICT      |    0        | 0        |   0        |   7        |
| REJOICE      |    0        | 0        |   0        |   3        |
| REQUEST      |    0.16     | 0.363636 |   0.222222 |  33        |
| REQUIRE      |    0        | 0        |   0        |  16        |
| SUGGEST      |    0        | 0        |   0        |   3        |
| SUSTAIN      |    0        | 0        |   0        |   3        |
| UNSURE       |    0        | 0        |   0        |  30        |
| WISH         |    0        | 0        |   0        |   2        |
| expressEMOJI |    0        | 0        |   0        |  21        |
| accuracy     |    0.247449 | 0.247449 |   0.247449 |   0.247449 |
| macro avg    |    0.134167 | 0.11147  |   0.100505 | 392        |
| weighted avg |    0.436462 | 0.247449 |   0.259602 | 392        |
### Hasoc2020 Dataset
#### Task Coarse labels
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| HOF          |    0.359184 | 0.656716 |   0.46438  |       134 |
| NOT          |    0.962025 | 0.581633 |   0.72496  |       392 |
| micro avg    |    0.655602 | 0.60076  |   0.626984 |       526 |
| macro avg    |    0.660604 | 0.619175 |   0.59467  |       526 |
| weighted avg |    0.80845  | 0.60076  |   0.658577 |       526 |
#### Task Fine labels
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| HATE         |    0        | 0        |   0        |        24 |
| NONE         |    0.978022 | 0.470899 |   0.635714 |       378 |
| OFFN         |    0.101818 | 0.777778 |   0.180064 |        36 |
| PRFN         |    0.433962 | 0.261364 |   0.326241 |        88 |
| micro avg    |    0.44902  | 0.435361 |   0.442085 |       526 |
| macro avg    |    0.378451 | 0.37751  |   0.285505 |       526 |
| weighted avg |    0.782408 | 0.435361 |   0.523748 |       526 |
### Germeval2019_task12 Dataset
#### Task Coarse labels
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| OFFENSE      |    0.408789 | 0.824742 |   0.546635 |       970 |
| OTHER        |    0.915074 | 0.418244 |   0.574093 |      2061 |
| micro avg    |    0.573301 | 0.548334 |   0.56054  |      3031 |
| macro avg    |    0.661932 | 0.621493 |   0.560364 |      3031 |
| weighted avg |    0.75305  | 0.548334 |   0.565305 |      3031 |
#### Task Fine labels
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| PROFANITY    |   0.0628019 | 0.117117 |   0.081761 |       111 |
| INSULT       |   0.310452  | 0.433551 |   0.361818 |       459 |
| ABUSE        |   0.21      | 0.2625   |   0.233333 |       400 |
| OTHER        |   0.872669  | 0.658418 |   0.750553 |      2061 |
| micro avg    |   0.576645  | 0.552293 |   0.564206 |      3031 |
| macro avg    |   0.363981  | 0.367897 |   0.356866 |      3031 |
| weighted avg |   0.670419  | 0.552293 |   0.598935 |      3031 |
### Germeval2019_task3 Dataset
#### Task Offensive labels
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| EXPLICIT     |    0.914729 | 0.148241 |   0.255135 |       796 |
| IMPLICIT     |    0.155303 | 0.91791  |   0.265659 |       134 |
| micro avg    |    0.261672 | 0.25914  |   0.2604   |       930 |
| macro avg    |    0.535016 | 0.533076 |   0.260397 |       930 |
| weighted avg |    0.805306 | 0.25914  |   0.256651 |       930 |
### Germeval2021 Dataset
#### Task Toxic
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| 0            |    0.717712 | 0.654882 |   0.684859 |       594 |
| 1            |    0.491003 | 0.545714 |   0.516915 |       350 |
| micro avg    |    0.622986 | 0.614407 |   0.618667 |       944 |
| macro avg    |    0.604357 | 0.600298 |   0.600887 |       944 |
| weighted avg |    0.633657 | 0.614407 |   0.622592 |       944 |
#### Task Engaging
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| 0            |    0.773585 | 0.593343 |   0.671581 |       691 |
| 1            |    0.321337 | 0.494071 |   0.389408 |       253 |
| micro avg    |    0.582155 | 0.566737 |   0.574342 |       944 |
| macro avg    |    0.547461 | 0.543707 |   0.530494 |       944 |
| weighted avg |    0.652379 | 0.566737 |   0.595956 |       944 |
#### Task Factclaiming
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| 0            |    0.847584 | 0.361905 |   0.50723  |       630 |
| 1            |    0.403587 | 0.859873 |   0.549339 |       314 |
| micro avg    |    0.530917 | 0.527542 |   0.529224 |       944 |
| macro avg    |    0.625586 | 0.610889 |   0.528285 |       944 |
| weighted avg |    0.699898 | 0.527542 |   0.521237 |       944 |
## EuroLLM-9B
### Covid19 Dataset
#### Task Informativeness
|                     |   precision |   recall |   f1-score |   support |
|:--------------------|------------:|---------:|-----------:|----------:|
| Informative         |    0.72381  | 0.873563 |   0.791667 |        87 |
| Personal_Experience |    0        | 0        |   0        |         7 |
| none                |    0.833333 | 0.142857 |   0.243902 |        35 |
| micro avg           |    0.72973  | 0.627907 |   0.675    |       129 |
| macro avg           |    0.519048 | 0.338807 |   0.34519  |       129 |
| weighted avg        |    0.714249 | 0.627907 |   0.60009  |       129 |
#### Task Topic
|                   |   precision |   recall |   f1-score |   support |
|:------------------|------------:|---------:|-----------:|----------:|
| Case_Report       |   0.4       | 0.108108 |   0.170213 |        37 |
| Consequences      |   0.0526316 | 0.166667 |   0.08     |         6 |
| Governm_Decisions |   0.5       | 0.227273 |   0.3125   |        22 |
| Risk_Reduction    |   0         | 0        |   0        |         3 |
| Vaccination       |   0.428571  | 0.157895 |   0.230769 |        19 |
| none              |   0.454545  | 0.357143 |   0.4      |        42 |
| micro avg         |   0.311111  | 0.217054 |   0.255708 |       129 |
| macro avg         |   0.305958  | 0.169514 |   0.198914 |       129 |
| weighted avg      |   0.413562  | 0.217054 |   0.270058 |       129 |
#### Task Credibility
|              |   precision |    recall |   f1-score |   support |
|:-------------|------------:|----------:|-----------:|----------:|
| credible     |    1        | 0.0641026 |  0.120482  |        78 |
| non-credible |    0        | 0         |  0         |         3 |
| none         |    0.4      | 0.0416667 |  0.0754717 |        48 |
| micro avg    |    0.538462 | 0.0542636 |  0.0985915 |       129 |
| macro avg    |    0.466667 | 0.0352564 |  0.0653179 |       129 |
| weighted avg |    0.753488 | 0.0542636 |  0.100932  |       129 |
### Speech_acts_coarse Dataset
#### Task Coarse labels
|              |   precision |    recall |   f1-score |   support |
|:-------------|------------:|----------:|-----------:|----------:|
| ASSERTIVE    |   0.242424  | 0.0583942 |  0.0941176 |       137 |
| COMMISSIVE   |   0.0163934 | 0.25      |  0.0307692 |         4 |
| DIRECTIVE    |   0.4125    | 0.259843  |  0.318841  |       127 |
| EXPRESSIVE   |   0.268519  | 0.3625    |  0.308511  |        80 |
| OTHER        |   0.1       | 0.285714  |  0.148148  |        14 |
| UNSURE       |   0.117647  | 0.133333  |  0.125     |        30 |
| micro avg    |   0.22191   | 0.201531  |  0.21123   |       392 |
| macro avg    |   0.192914  | 0.224964  |  0.170898  |       392 |
| weighted avg |   0.285908  | 0.201531  |  0.214324  |       392 |
### Speech_acts_fine Dataset
#### Task Fine labels
|              |   precision |    recall |   f1-score |   support |
|:-------------|------------:|----------:|-----------:|----------:|
| ADDRESS      |    0.725806 | 0.6       |   0.656934 |        75 |
| AGREE        |    0        | 0         |   0        |         3 |
| ASSERT       |    0.611111 | 0.0932203 |   0.161765 |       118 |
| COMMISSIVE   |    0        | 0         |   0        |         4 |
| COMPLAIN     |    0.251656 | 0.745098  |   0.376238 |        51 |
| EXCLUDED     |    0        | 0         |   0        |         3 |
| GUESS        |    0        | 0         |   0        |         5 |
| OTHER        |    0        | 0         |   0        |        15 |
| PREDICT      |    0        | 0         |   0        |         7 |
| REJOICE      |    0        | 0         |   0        |         3 |
| REQUEST      |    0.428571 | 0.0909091 |   0.15     |        33 |
| REQUIRE      |    0        | 0         |   0        |        16 |
| SUGGEST      |    0        | 0         |   0        |         3 |
| SUSTAIN      |    0        | 0         |   0        |         3 |
| UNSURE       |    0        | 0         |   0        |        30 |
| WISH         |    0        | 0         |   0        |         2 |
| expressEMOJI |    0.65     | 0.619048  |   0.634146 |        21 |
| micro avg    |    0.307263 | 0.280612  |   0.293333 |       392 |
| macro avg    |    0.156891 | 0.126369  |   0.116417 |       392 |
| weighted avg |    0.426464 | 0.280612  |   0.269932 |       392 |
### Hasoc2020 Dataset
#### Task Coarse labels
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| HOF          |    0.365591 | 0.507463 |   0.425    |       134 |
| NOT          |    0.831579 | 0.201531 |   0.324435 |       392 |
| micro avg    |    0.523132 | 0.279468 |   0.364312 |       526 |
| macro avg    |    0.598585 | 0.354497 |   0.374718 |       526 |
| weighted avg |    0.712867 | 0.279468 |   0.350054 |       526 |
#### Task Fine labels
|              |   precision |    recall |   f1-score |   support |
|:-------------|------------:|----------:|-----------:|----------:|
| HATE         |   0.142857  | 0.0833333 |  0.105263  |        24 |
| NONE         |   0.88172   | 0.216931  |  0.348195  |       378 |
| OFFN         |   0.0890411 | 0.722222  |  0.158537  |        36 |
| PRFN         |   0.166667  | 0.0340909 |  0.0566038 |        88 |
| micro avg    |   0.270983  | 0.214829  |  0.239661  |       526 |
| macro avg    |   0.320071  | 0.264144  |  0.16715   |       526 |
| weighted avg |   0.674127  | 0.214829  |  0.275347  |       526 |
### Germeval2019_task12 Dataset
#### Task Coarse labels
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| OFFENSE      |    0.4256   | 0.82268  |   0.560984 |       970 |
| OTHER        |    0.87458  | 0.378942 |   0.528775 |      2061 |
| micro avg    |    0.570448 | 0.52095  |   0.544577 |      3031 |
| macro avg    |    0.65009  | 0.600811 |   0.544879 |      3031 |
| weighted avg |    0.730895 | 0.52095  |   0.539082 |      3031 |
#### Task Fine labels
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| PROFANITY    |   0.0567376 | 0.288288 |  0.0948148 |       111 |
| INSULT       |   0.203159  | 0.616558 |  0.305616  |       459 |
| ABUSE        |   0.208092  | 0.09     |  0.125654  |       400 |
| OTHER        |   0.876518  | 0.210092 |  0.338943  |      2061 |
| micro avg    |   0.29878   | 0.258661 |  0.277277  |      3031 |
| macro avg    |   0.336127  | 0.301235 |  0.216257  |      3031 |
| weighted avg |   0.656314  | 0.258661 |  0.296808  |      3031 |
### Germeval2019_task3 Dataset
#### Task Offensive labels
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| EXPLICIT     |    0.860606 | 0.178392 |   0.295525 |       796 |
| IMPLICIT     |    0.141856 | 0.604478 |   0.229787 |       134 |
| micro avg    |    0.302989 | 0.239785 |   0.267707 |       930 |
| macro avg    |    0.501231 | 0.391435 |   0.262656 |       930 |
| weighted avg |    0.757044 | 0.239785 |   0.286054 |       930 |
### Germeval2021 Dataset
#### Task Toxic
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| 0            |    0.738462 | 0.161616 |   0.265193 |       594 |
| 1            |    0.393258 | 0.8      |   0.527307 |       350 |
| micro avg    |    0.446556 | 0.398305 |   0.421053 |       944 |
| macro avg    |    0.56586  | 0.480808 |   0.39625  |       944 |
| weighted avg |    0.610473 | 0.398305 |   0.362375 |       944 |
#### Task Engaging
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| 0            |    0.791277 | 0.367583 |   0.501976 |       691 |
| 1            |    0.325    | 0.616601 |   0.425648 |       253 |
| micro avg    |    0.51186  | 0.434322 |   0.469914 |       944 |
| macro avg    |    0.558139 | 0.492092 |   0.463812 |       944 |
| weighted avg |    0.666311 | 0.434322 |   0.48152  |       944 |
#### Task Factclaiming
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| 0            |    0.797753 | 0.112698 |   0.197497 |       630 |
| 1            |    0.345006 | 0.847134 |   0.490323 |       314 |
| micro avg    |    0.39186  | 0.356992 |   0.373614 |       944 |
| macro avg    |    0.57138  | 0.479916 |   0.34391  |       944 |
| weighted avg |    0.647157 | 0.356992 |   0.294898 |       944 |
## Teuken-7B
### Covid19 Dataset
#### Task Informativeness
|                     |   precision |   recall |   f1-score |    support |
|:--------------------|------------:|---------:|-----------:|-----------:|
| Informative         |    0.790909 | 1        |   0.883249 |  87        |
| Personal_Experience |    0.230769 | 0.428571 |   0.3      |   7        |
| none                |    1        | 0.171429 |   0.292683 |  35        |
| accuracy            |    0.744186 | 0.744186 |   0.744186 |   0.744186 |
| macro avg           |    0.673893 | 0.533333 |   0.491977 | 129        |
| weighted avg        |    0.817244 | 0.744186 |   0.691369 | 129        |
#### Task Topic
|                   |   precision |   recall |   f1-score |   support |
|:------------------|------------:|---------:|-----------:|----------:|
| Case_Report       |    0.875    | 0.189189 |   0.311111 |        37 |
| Consequences      |    0.138889 | 0.833333 |   0.238095 |         6 |
| Governm_Decisions |    0.8      | 0.181818 |   0.296296 |        22 |
| Risk_Reduction    |    0        | 0        |   0        |         3 |
| Vaccination       |    0.451613 | 0.736842 |   0.56     |        19 |
| none              |    0.5      | 0.285714 |   0.363636 |        42 |
| micro avg         |    0.35     | 0.325581 |   0.337349 |       129 |
| macro avg         |    0.460917 | 0.37115  |   0.294857 |       129 |
| weighted avg      |    0.62317  | 0.325581 |   0.351713 |       129 |
#### Task Credibility
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| credible     |    0.795918 | 0.5      |   0.614173 |  78        |
| non-credible |    0        | 0        |   0        |   3        |
| none         |    0.438356 | 0.666667 |   0.528926 |  48        |
| accuracy     |    0.550388 | 0.550388 |   0.550388 |   0.550388 |
| macro avg    |    0.411425 | 0.388889 |   0.381033 | 129        |
| weighted avg |    0.644362 | 0.550388 |   0.56817  | 129        |
### Speech_acts_coarse Dataset
#### Task Coarse labels
|              |   precision |    recall |   f1-score |   support |
|:-------------|------------:|----------:|-----------:|----------:|
| ASSERTIVE    |   0.377778  | 0.992701  |  0.547284  |       137 |
| COMMISSIVE   |   0         | 0         |  0         |         4 |
| DIRECTIVE    |   0         | 0         |  0         |       127 |
| EXPRESSIVE   |   1         | 0.0125    |  0.0246914 |        80 |
| OTHER        |   0.0769231 | 0.0714286 |  0.0740741 |        14 |
| UNSURE       |   0.0666667 | 0.0333333 |  0.0444444 |        30 |
| micro avg    |   0.357326  | 0.354592  |  0.355954  |       392 |
| macro avg    |   0.253561  | 0.184994  |  0.115082  |       392 |
| weighted avg |   0.34396   | 0.354592  |  0.202356  |       392 |
### Speech_acts_fine Dataset
#### Task Fine labels
|              |   precision |    recall |   f1-score |   support |
|:-------------|------------:|----------:|-----------:|----------:|
| ADDRESS      |   0.352941  | 0.16      |  0.220183  |        75 |
| AGREE        |   0         | 0         |  0         |         3 |
| ASSERT       |   0         | 0         |  0         |       118 |
| COMMISSIVE   |   0         | 0         |  0         |         4 |
| COMPLAIN     |   0         | 0         |  0         |        51 |
| EXCLUDED     |   0         | 0         |  0         |         3 |
| GUESS        |   0         | 0         |  0         |         5 |
| OTHER        |   0.0562771 | 0.866667  |  0.105691  |        15 |
| PREDICT      |   0.0666667 | 0.714286  |  0.121951  |         7 |
| REJOICE      |   0         | 0         |  0         |         3 |
| REQUEST      |   0         | 0         |  0         |        33 |
| REQUIRE      |   0.1       | 0.0625    |  0.0769231 |        16 |
| SUGGEST      |   0         | 0         |  0         |         3 |
| SUSTAIN      |   0         | 0         |  0         |         3 |
| UNSURE       |   0         | 0         |  0         |        30 |
| WISH         |   0         | 0         |  0         |         2 |
| expressEMOJI |   0         | 0         |  0         |        21 |
| micro avg    |   0.0826667 | 0.0790816 |  0.0808344 |       392 |
| macro avg    |   0.0338756 | 0.106085  |  0.0308676 |       392 |
| weighted avg |   0.0749526 | 0.0790816 |  0.0514887 |       392 |
### Hasoc2020 Dataset
#### Task Coarse labels
|              |   precision |    recall |   f1-score |   support |
|:-------------|------------:|----------:|-----------:|----------:|
| HOF          |    0.257692 | 1         |  0.409786  | 134       |
| NOT          |    1        | 0.0153061 |  0.0301508 | 392       |
| accuracy     |    0.26616  | 0.26616   |  0.26616   |   0.26616 |
| macro avg    |    0.628846 | 0.507653  |  0.219968  | 526       |
| weighted avg |    0.810895 | 0.26616   |  0.126864  | 526       |
#### Task Fine labels
|              |   precision |    recall |   f1-score |   support |
|:-------------|------------:|----------:|-----------:|----------:|
| HATE         |   0         | 0         |  0         |        24 |
| NONE         |   0.965517  | 0.0740741 |  0.137592  |       378 |
| OFFN         |   0.0727273 | 1         |  0.135593  |        36 |
| PRFN         |   0         | 0         |  0         |        88 |
| micro avg    |   0.121905  | 0.121673  |  0.121789  |       526 |
| macro avg    |   0.259561  | 0.268519  |  0.0682963 |       526 |
| weighted avg |   0.698828  | 0.121673  |  0.108158  |       526 |
### Germeval2019_task12 Dataset
#### Task Coarse labels
|              |   precision |   recall |   f1-score |     support |
|:-------------|------------:|---------:|-----------:|------------:|
| OFFENSE      |    0.320026 | 1        |   0.484879 |  970        |
| OTHER        |    0        | 0        |   0        | 2061        |
| accuracy     |    0.320026 | 0.320026 |   0.320026 |    0.320026 |
| macro avg    |    0.160013 | 0.5      |   0.242439 | 3031        |
| weighted avg |    0.102417 | 0.320026 |   0.155174 | 3031        |
#### Task Fine labels
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| PROFANITY    |   0.0512821 | 0.018018 |  0.0266667 |       111 |
| INSULT       |   0.18789   | 0.91939  |  0.312015  |       459 |
| ABUSE        |   0         | 0        |  0         |       400 |
| OTHER        |   0.929348  | 0.331878 |  0.489095  |      2061 |
| micro avg    |   0.36616   | 0.365556 |  0.365858  |      3031 |
| macro avg    |   0.29213   | 0.317321 |  0.206944  |      3031 |
| weighted avg |   0.662263  | 0.365556 |  0.380799  |      3031 |
### Germeval2019_task3 Dataset
#### Task Offensive labels
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| EXPLICIT     |    0.855914 | 1        |   0.922364 | 796        |
| IMPLICIT     |    0        | 0        |   0        | 134        |
| accuracy     |    0.855914 | 0.855914 |   0.855914 |   0.855914 |
| macro avg    |    0.427957 | 0.5      |   0.461182 | 930        |
| weighted avg |    0.732589 | 0.855914 |   0.789464 | 930        |
### Germeval2021 Dataset
#### Task Toxic
|              |   precision |    recall |   f1-score |   support |
|:-------------|------------:|----------:|-----------:|----------:|
| 0            |    0.84058  | 0.0976431 |   0.174962 |       594 |
| 1            |    0.389401 | 0.965714  |   0.555008 |       350 |
| micro avg    |    0.422625 | 0.419492  |   0.421053 |       944 |
| macro avg    |    0.61499  | 0.531679  |   0.364985 |       944 |
| weighted avg |    0.673299 | 0.419492  |   0.315869 |       944 |
#### Task Engaging
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| 0            |    0.831776 | 0.128799 |   0.223058 |       691 |
| 1            |    0.281896 | 0.916996 |   0.431227 |       253 |
| micro avg    |    0.345161 | 0.340042 |   0.342583 |       944 |
| macro avg    |    0.556836 | 0.522897 |   0.327142 |       944 |
| weighted avg |    0.684403 | 0.340042 |   0.278849 |       944 |
#### Task Factclaiming
|              |   precision |     recall |   f1-score |   support |
|:-------------|------------:|-----------:|-----------:|----------:|
| 0            |    1        | 0.00634921 |  0.0126183 |       630 |
| 1            |    0.333333 | 0.996815   |  0.499601  |       314 |
| micro avg    |    0.336161 | 0.335805   |  0.335983  |       944 |
| macro avg    |    0.666667 | 0.501582   |  0.25611   |       944 |
| weighted avg |    0.778249 | 0.335805   |  0.174602  |       944 |
# Few-Shot Classification
## Llama-3.2-3B
### Covid19 Dataset
#### Task Informativeness
|                     |   precision |   recall |   f1-score |    support |
|:--------------------|------------:|---------:|-----------:|-----------:|
| Informative         |    0.931507 | 0.781609 |   0.85     |  87        |
| Personal_Experience |    0        | 0        |   0        |   7        |
| none                |    0.571429 | 0.914286 |   0.703297 |  35        |
| accuracy            |    0.775194 | 0.775194 |   0.775194 |   0.775194 |
| macro avg           |    0.500978 | 0.565298 |   0.517766 | 129        |
| weighted avg        |    0.783264 | 0.775194 |   0.764073 | 129        |
#### Task Topic
|                   |   precision |   recall |   f1-score |   support |
|:------------------|------------:|---------:|-----------:|----------:|
| Case_Report       |    0.809524 | 0.459459 |   0.586207 |        37 |
| Consequences      |    0.16129  | 0.833333 |   0.27027  |         6 |
| Governm_Decisions |    0.666667 | 0.272727 |   0.387097 |        22 |
| Risk_Reduction    |    0        | 0        |   0        |         3 |
| Vaccination       |    0.888889 | 0.421053 |   0.571429 |        19 |
| none              |    0.509091 | 0.666667 |   0.57732  |        42 |
| micro avg         |    0.507937 | 0.496124 |   0.501961 |       129 |
| macro avg         |    0.50591  | 0.442207 |   0.39872  |       129 |
| weighted avg      |    0.650058 | 0.496124 |   0.518852 |       129 |
#### Task Credibility
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| credible     |    1        | 0.448718 |   0.619469 |        78 |
| non-credible |    0        | 0        |   0        |         3 |
| none         |    0.612245 | 0.625    |   0.618557 |        48 |
| micro avg    |    0.656566 | 0.503876 |   0.570175 |       129 |
| macro avg    |    0.537415 | 0.357906 |   0.412675 |       129 |
| weighted avg |    0.832463 | 0.503876 |   0.604723 |       129 |
### Speech_acts_coarse Dataset
#### Task Coarse labels
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| ASSERTIVE    |   0.522727  | 0.335766 |   0.408889 |       137 |
| COMMISSIVE   |   0.333333  | 0.25     |   0.285714 |         4 |
| DIRECTIVE    |   0.561404  | 0.503937 |   0.53112  |       127 |
| EXPRESSIVE   |   0.857143  | 0.075    |   0.137931 |        80 |
| OTHER        |   0.170732  | 0.5      |   0.254545 |        14 |
| UNSURE       |   0.0650407 | 0.266667 |   0.104575 |        30 |
| micro avg    |   0.351064  | 0.336735 |   0.34375  |       392 |
| macro avg    |   0.418397  | 0.321895 |   0.287129 |       392 |
| weighted avg |   0.553975  | 0.336735 |   0.363133 |       392 |
### Speech_acts_fine Dataset
#### Task Fine labels
|              |   precision |    recall |   f1-score |   support |
|:-------------|------------:|----------:|-----------:|----------:|
| ADDRESS      |   1         | 0.573333  |  0.728814  |        75 |
| AGREE        |   0.027027  | 0.666667  |  0.0519481 |         3 |
| ASSERT       |   0.5625    | 0.0762712 |  0.134328  |       118 |
| COMMISSIVE   |   1         | 0.25      |  0.4       |         4 |
| COMPLAIN     |   0.355932  | 0.411765  |  0.381818  |        51 |
| EXCLUDED     |   0         | 0         |  0         |         3 |
| GUESS        |   0         | 0         |  0         |         5 |
| OTHER        |   0.45      | 0.6       |  0.514286  |        15 |
| PREDICT      |   0         | 0         |  0         |         7 |
| REJOICE      |   0.0192308 | 0.333333  |  0.0363636 |         3 |
| REQUEST      |   0         | 0         |  0         |        33 |
| REQUIRE      |   0.25      | 0.0625    |  0.1       |        16 |
| SUGGEST      |   1         | 0.333333  |  0.5       |         3 |
| SUSTAIN      |   0         | 0         |  0         |         3 |
| UNSURE       |   0.0612245 | 0.1       |  0.0759494 |        30 |
| WISH         |   0         | 0         |  0         |         2 |
| expressEMOJI |   0         | 0         |  0         |        21 |
| micro avg    |   0.233933  | 0.232143  |  0.233035  |       392 |
| macro avg    |   0.277995  | 0.200424  |  0.171971  |       392 |
| weighted avg |   0.457278  | 0.232143  |  0.26771   |       392 |
### Hasoc2020 Dataset
#### Task Coarse labels
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| HOF          |    0.483444 | 0.544776 |   0.512281 |       134 |
| NOT          |    0.914894 | 0.438776 |   0.593103 |       392 |
| micro avg    |    0.722714 | 0.465779 |   0.566474 |       526 |
| macro avg    |    0.699169 | 0.491776 |   0.552692 |       526 |
| weighted avg |    0.804981 | 0.465779 |   0.572514 |       526 |
#### Task Fine labels
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| HATE         |   0         | 0        |   0        |        24 |
| NONE         |   0.968254  | 0.322751 |   0.484127 |       378 |
| OFFN         |   0.0934579 | 0.277778 |   0.13986  |        36 |
| PRFN         |   0.504673  | 0.613636 |   0.553846 |        88 |
| micro avg    |   0.536023  | 0.353612 |   0.426117 |       526 |
| macro avg    |   0.391596  | 0.303541 |   0.294458 |       526 |
| weighted avg |   0.786646  | 0.353612 |   0.45014  |       526 |
### Germeval2019_task12 Dataset
#### Task Coarse labels
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| OFFENSE      |    0.367592 | 0.914433 |   0.524387 |       970 |
| OTHER        |    0.935484 | 0.084425 |   0.154873 |      2061 |
| micro avg    |    0.408234 | 0.350049 |   0.376909 |      3031 |
| macro avg    |    0.651538 | 0.499429 |   0.33963  |      3031 |
| weighted avg |    0.753744 | 0.350049 |   0.273127 |      3031 |
#### Task Fine labels
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| PROFANITY    |   0.0981013 | 0.279279 |  0.145199  |       111 |
| INSULT       |   0.392157  | 0.217865 |  0.280112  |       459 |
| ABUSE        |   0.325     | 0.0325   |  0.0590909 |       400 |
| OTHER        |   0.817547  | 0.510917 |  0.628844  |      2061 |
| micro avg    |   0.630332  | 0.394919 |  0.485598  |      3031 |
| macro avg    |   0.408201  | 0.26014  |  0.278312  |      3031 |
| weighted avg |   0.661779  | 0.394919 |  0.483132  |      3031 |
### Germeval2019_task3 Dataset
#### Task Offensive labels
|              |   precision |    recall |   f1-score |   support |
|:-------------|------------:|----------:|-----------:|----------:|
| EXPLICIT     |    1        | 0.0175879 |  0.0345679 | 796       |
| IMPLICIT     |    0.146288 | 1         |  0.255238  | 134       |
| accuracy     |    0.15914  | 0.15914   |  0.15914   |   0.15914 |
| macro avg    |    0.573144 | 0.508794  |  0.144903  | 930       |
| weighted avg |    0.876992 | 0.15914   |  0.0663634 | 930       |
### Germeval2021 Dataset
#### Task Toxic
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| 0            |    0.719335 | 0.582492 |   0.643721 | 594       |
| 1            |    0.464363 | 0.614286 |   0.528905 | 350       |
| accuracy     |    0.59428  | 0.59428  |   0.59428  |   0.59428 |
| macro avg    |    0.591849 | 0.598389 |   0.586313 | 944       |
| weighted avg |    0.624801 | 0.59428  |   0.601152 | 944       |
#### Task Engaging
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| 0            |    0.744681 | 0.40521  |   0.524836 |       691 |
| 1            |    0.276762 | 0.418972 |   0.333333 |       253 |
| micro avg    |    0.508564 | 0.408898 |   0.453318 |       944 |
| macro avg    |    0.510722 | 0.412091 |   0.429085 |       944 |
| weighted avg |    0.619275 | 0.408898 |   0.473512 |       944 |
#### Task Factclaiming
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| 0            |    0.774678 | 0.573016 |   0.658759 | 630        |
| 1            |    0.437238 | 0.665605 |   0.527778 | 314        |
| accuracy     |    0.603814 | 0.603814 |   0.603814 |   0.603814 |
| macro avg    |    0.605958 | 0.61931  |   0.593268 | 944        |
| weighted avg |    0.662437 | 0.603814 |   0.615191 | 944        |
## EuroLLM-9B
### Covid19 Dataset
#### Task Informativeness
|                     |   precision |   recall |   f1-score |   support |
|:--------------------|------------:|---------:|-----------:|----------:|
| Informative         |    0.819149 | 0.885057 |   0.850829 |        87 |
| Personal_Experience |    0.666667 | 0.285714 |   0.4      |         7 |
| none                |    0.76     | 0.542857 |   0.633333 |        35 |
| micro avg           |    0.803279 | 0.75969  |   0.780876 |       129 |
| macro avg           |    0.748605 | 0.57121  |   0.628054 |       129 |
| weighted avg        |    0.794827 | 0.75969  |   0.767355 |       129 |
#### Task Topic
|                   |   precision |    recall |   f1-score |   support |
|:------------------|------------:|----------:|-----------:|----------:|
| Case_Report       |   0.333333  | 0.108108  |  0.163265  |        37 |
| Consequences      |   0.0851064 | 0.666667  |  0.150943  |         6 |
| Governm_Decisions |   0.142857  | 0.0454545 |  0.0689655 |        22 |
| Risk_Reduction    |   0         | 0         |  0         |         3 |
| Vaccination       |   0.478261  | 0.578947  |  0.52381   |        19 |
| none              |   0.4       | 0.047619  |  0.0851064 |        42 |
| micro avg         |   0.201835  | 0.170543  |  0.184874  |       129 |
| macro avg         |   0.239926  | 0.241133  |  0.165348  |       129 |
| weighted avg      |   0.324603  | 0.170543  |  0.17047   |       129 |
#### Task Credibility
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| credible     |   0.830508  | 0.628205 |   0.715328 |        78 |
| non-credible |   0.0606061 | 0.666667 |   0.111111 |         3 |
| none         |   0.5625    | 0.1875   |   0.28125  |        48 |
| micro avg    |   0.555556  | 0.465116 |   0.506329 |       129 |
| macro avg    |   0.484538  | 0.494124 |   0.36923  |       129 |
| weighted avg |   0.71288   | 0.465116 |   0.539759 |       129 |
### Speech_acts_coarse Dataset
#### Task Coarse labels
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| ASSERTIVE    |   0.426667  | 0.233577 |  0.301887  |       137 |
| COMMISSIVE   |   0.0133333 | 0.25     |  0.0253165 |         4 |
| DIRECTIVE    |   0.3       | 0.141732 |  0.192513  |       127 |
| EXPRESSIVE   |   0.474359  | 0.4625   |  0.468354  |        80 |
| OTHER        |   0.0357143 | 0.142857 |  0.0571429 |        14 |
| UNSURE       |   0         | 0        |  0         |        30 |
| micro avg    |   0.25      | 0.229592 |  0.239362  |       392 |
| macro avg    |   0.208346  | 0.205111 |  0.174202  |       392 |
| weighted avg |   0.344529  | 0.229592 |  0.265758  |       392 |
### Speech_acts_fine Dataset
#### Task Fine labels
|              |   precision |    recall |   f1-score |   support |
|:-------------|------------:|----------:|-----------:|----------:|
| ADDRESS      |   0.25      | 0.04      |  0.0689655 |        75 |
| AGREE        |   0.047619  | 0.333333  |  0.0833333 |         3 |
| ASSERT       |   0.4       | 0.0338983 |  0.0625    |       118 |
| COMMISSIVE   |   0         | 0         |  0         |         4 |
| COMPLAIN     |   0.256757  | 0.372549  |  0.304     |        51 |
| EXCLUDED     |   0.0645161 | 0.666667  |  0.117647  |         3 |
| GUESS        |   0         | 0         |  0         |         5 |
| OTHER        |   0         | 0         |  0         |        15 |
| PREDICT      |   0.0333333 | 0.142857  |  0.0540541 |         7 |
| REJOICE      |   0         | 0         |  0         |         3 |
| REQUEST      |   0.0869565 | 0.0606061 |  0.0714286 |        33 |
| REQUIRE      |   0         | 0         |  0         |        16 |
| SUGGEST      |   0         | 0         |  0         |         3 |
| SUSTAIN      |   0         | 0         |  0         |         3 |
| UNSURE       |   0.111111  | 0.1       |  0.105263  |        30 |
| WISH         |   0         | 0         |  0         |         2 |
| expressEMOJI |   0.416667  | 0.238095  |  0.30303   |        21 |
| micro avg    |   0.118694  | 0.102041  |  0.109739  |       392 |
| macro avg    |   0.0980564 | 0.116942  |  0.0688366 |       392 |
| weighted avg |   0.241243  | 0.102041  |  0.104366  |       392 |
### Hasoc2020 Dataset
#### Task Coarse labels
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| HOF          |    0.413793 | 0.626866 |   0.498516 |       134 |
| NOT          |    0.852941 | 0.591837 |   0.698795 |       392 |
| micro avg    |    0.665263 | 0.60076  |   0.631369 |       526 |
| macro avg    |    0.633367 | 0.609351 |   0.598656 |       526 |
| weighted avg |    0.741067 | 0.60076  |   0.647774 |       526 |
#### Task Fine labels
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| HATE         |   0.108696  | 0.208333 |   0.142857 |        24 |
| NONE         |   0.930693  | 0.248677 |   0.392484 |       378 |
| OFFN         |   0.0687285 | 0.555556 |   0.122324 |        36 |
| PRFN         |   0.367347  | 0.204545 |   0.262774 |        88 |
| micro avg    |   0.281314  | 0.260456 |   0.270484 |       526 |
| macro avg    |   0.368866  | 0.304278 |   0.23011  |       526 |
| weighted avg |   0.739946  | 0.260456 |   0.340904 |       526 |
### Germeval2019_task12 Dataset
#### Task Coarse labels
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| OFFENSE      |    0.482422 | 0.763918 |   0.591381 |       970 |
| OTHER        |    0.864865 | 0.574478 |   0.690379 |      2061 |
| micro avg    |    0.662651 | 0.635104 |   0.648585 |      3031 |
| macro avg    |    0.673643 | 0.669198 |   0.64088  |      3031 |
| weighted avg |    0.742473 | 0.635104 |   0.658697 |      3031 |
#### Task Fine labels
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| PROFANITY    |   0.0627178 | 0.162162 |  0.0904523 |       111 |
| INSULT       |   0.166302  | 0.496732 |  0.24918   |       459 |
| ABUSE        |   0.231579  | 0.33     |  0.272165  |       400 |
| OTHER        |   0.868421  | 0.176128 |  0.29286   |      2061 |
| micro avg    |   0.280045  | 0.244474 |  0.261053  |      3031 |
| macro avg    |   0.332255  | 0.291256 |  0.226164  |      3031 |
| weighted avg |   0.648546  | 0.244474 |  0.276102  |      3031 |
### Germeval2019_task3 Dataset
#### Task Offensive labels
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| EXPLICIT     |    0.906015 | 0.302764 |   0.453861 |       796 |
| IMPLICIT     |    0.16158  | 0.671642 |   0.260492 |       134 |
| micro avg    |    0.402187 | 0.355914 |   0.377638 |       930 |
| macro avg    |    0.533797 | 0.487203 |   0.357176 |       930 |
| weighted avg |    0.798752 | 0.355914 |   0.425999 |       930 |
### Germeval2021 Dataset
#### Task Toxic
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| 0            |    0.68071  | 0.516835 |   0.58756  |       594 |
| 1            |    0.435967 | 0.457143 |   0.446304 |       350 |
| micro avg    |    0.570905 | 0.494703 |   0.530079 |       944 |
| macro avg    |    0.558338 | 0.486989 |   0.516932 |       944 |
| weighted avg |    0.589968 | 0.494703 |   0.535187 |       944 |
#### Task Engaging
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| 0            |    0.774704 | 0.567294 |   0.654971 |       691 |
| 1            |    0.287449 | 0.280632 |   0.284    |       253 |
| micro avg    |    0.614874 | 0.490466 |   0.545669 |       944 |
| macro avg    |    0.531076 | 0.423963 |   0.469485 |       944 |
| weighted avg |    0.644115 | 0.490466 |   0.555547 |       944 |
#### Task Factclaiming
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| 0            |    0.727955 | 0.615873 |   0.66724  |       630 |
| 1            |    0.441176 | 0.382166 |   0.409556 |       314 |
| micro avg    |    0.631056 | 0.538136 |   0.580903 |       944 |
| macro avg    |    0.584566 | 0.499019 |   0.538398 |       944 |
| weighted avg |    0.632565 | 0.538136 |   0.581527 |       944 |
## Teuken-7B
### Covid19 Dataset
#### Task Informativeness
|                     |   precision |   recall |   f1-score |   support |
|:--------------------|------------:|---------:|-----------:|----------:|
| Informative         |    0.89011  | 0.931034 |   0.910112 |        87 |
| Personal_Experience |    0.4      | 0.285714 |   0.333333 |         7 |
| none                |    0.75     | 0.685714 |   0.716418 |        35 |
| micro avg           |    0.835938 | 0.829457 |   0.832685 |       129 |
| macro avg           |    0.680037 | 0.634154 |   0.653288 |       129 |
| weighted avg        |    0.8255   | 0.829457 |   0.826262 |       129 |
#### Task Topic
|                   |   precision |   recall |   f1-score |   support |
|:------------------|------------:|---------:|-----------:|----------:|
| Case_Report       |   1         | 0.027027 |  0.0526316 |        37 |
| Consequences      |   0.4       | 0.333333 |  0.363636  |         6 |
| Governm_Decisions |   0.666667  | 0.181818 |  0.285714  |        22 |
| Risk_Reduction    |   0.0357143 | 0.666667 |  0.0677966 |         3 |
| Vaccination       |   1         | 0.263158 |  0.416667  |        19 |
| none              |   0.622642  | 0.785714 |  0.694737  |        42 |
| micro avg         |   0.373016  | 0.364341 |  0.368627  |       129 |
| macro avg         |   0.620837  | 0.376286 |  0.31353   |       129 |
| weighted avg      |   0.769959  | 0.364341 |  0.369875  |       129 |
#### Task Credibility
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| credible     |    0.913043 | 0.269231 |   0.415842 |  78        |
| non-credible |    0.333333 | 0.333333 |   0.333333 |   3        |
| none         |    0.427184 | 0.916667 |   0.582781 |  48        |
| accuracy     |    0.511628 | 0.511628 |   0.511628 |   0.511628 |
| macro avg    |    0.557854 | 0.50641  |   0.443985 | 129        |
| weighted avg |    0.718777 | 0.511628 |   0.47604  | 129        |
### Speech_acts_coarse Dataset
#### Task Coarse labels
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| ASSERTIVE    |   0.413043  | 0.832117 |  0.552058  | 137        |
| COMMISSIVE   |   0         | 0        |  0         |   4        |
| DIRECTIVE    |   0         | 0        |  0         | 127        |
| EXPRESSIVE   |   0         | 0        |  0         |  80        |
| OTHER        |   0.666667  | 0.428571 |  0.521739  |  14        |
| UNSURE       |   0.0480769 | 0.166667 |  0.0746269 |  30        |
| accuracy     |   0.318878  | 0.318878 |  0.318878  |   0.318878 |
| macro avg    |   0.187965  | 0.237892 |  0.191404  | 392        |
| weighted avg |   0.171843  | 0.318878 |  0.217283  | 392        |
### Speech_acts_fine Dataset
#### Task Fine labels
|              |   precision |    recall |   f1-score |   support |
|:-------------|------------:|----------:|-----------:|----------:|
| ADDRESS      |   0.645455  | 0.946667  |  0.767568  |        75 |
| AGREE        |   0         | 0         |  0         |         3 |
| ASSERT       |   0         | 0         |  0         |       118 |
| COMMISSIVE   |   0         | 0         |  0         |         4 |
| COMPLAIN     |   0         | 0         |  0         |        51 |
| EXCLUDED     |   0         | 0         |  0         |         3 |
| GUESS        |   0         | 0         |  0         |         5 |
| OTHER        |   0.0697674 | 0.4       |  0.118812  |        15 |
| PREDICT      |   0.0472973 | 1         |  0.0903226 |         7 |
| REJOICE      |   0         | 0         |  0         |         3 |
| REQUEST      |   0.333333  | 0.030303  |  0.0555556 |        33 |
| REQUIRE      |   0         | 0         |  0         |        16 |
| SUGGEST      |   0         | 0         |  0         |         3 |
| SUSTAIN      |   0         | 0         |  0         |         3 |
| UNSURE       |   0.0909091 | 0.0333333 |  0.0487805 |        30 |
| WISH         |   0         | 0         |  0         |         2 |
| expressEMOJI |   0.625     | 0.47619   |  0.540541  |        21 |
| micro avg    |   0.246154  | 0.244898  |  0.245524  |       392 |
| macro avg    |   0.106574  | 0.169794  |  0.095387  |       392 |
| weighted avg |   0.195508  | 0.244898  |  0.190383  |       392 |
### Hasoc2020 Dataset
#### Task Coarse labels
|              |   precision |    recall |   f1-score |    support |
|:-------------|------------:|----------:|-----------:|-----------:|
| HOF          |    0.261209 | 1         |  0.414219  | 134        |
| NOT          |    1        | 0.0331633 |  0.0641975 | 392        |
| accuracy     |    0.279468 | 0.279468  |  0.279468  |   0.279468 |
| macro avg    |    0.630604 | 0.516582  |  0.239209  | 526        |
| weighted avg |    0.811791 | 0.279468  |  0.153367  | 526        |
#### Task Fine labels
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| HATE         |    0.181818 | 0.166667 |   0.173913 |        24 |
| NONE         |    0.735471 | 0.970899 |   0.836944 |       378 |
| OFFN         |    0        | 0        |   0        |        36 |
| PRFN         |    0        | 0        |   0        |        88 |
| micro avg    |    0.708015 | 0.705323 |   0.706667 |       526 |
| macro avg    |    0.229322 | 0.284392 |   0.252714 |       526 |
| weighted avg |    0.536828 | 0.705323 |   0.609389 |       526 |
### Germeval2019_task12 Dataset
#### Task Coarse labels
|              |   precision |   recall |   f1-score |     support |
|:-------------|------------:|---------:|-----------:|------------:|
| OFFENSE      |    0.320026 | 1        |   0.484879 |  970        |
| OTHER        |    0        | 0        |   0        | 2061        |
| accuracy     |    0.320026 | 0.320026 |   0.320026 |    0.320026 |
| macro avg    |    0.160013 | 0.5      |   0.242439 | 3031        |
| weighted avg |    0.102417 | 0.320026 |   0.155174 | 3031        |
#### Task Fine labels
|              |   precision |     recall |   f1-score |   support |
|:-------------|------------:|-----------:|-----------:|----------:|
| PROFANITY    |   0.0378751 | 0.693694   | 0.0718284  |       111 |
| INSULT       |   0.205628  | 0.413943   | 0.274765   |       459 |
| ABUSE        |   0.363636  | 0.06       | 0.103004   |       400 |
| OTHER        |   1         | 0.00194081 | 0.00387409 |      2061 |
| micro avg    |   0.0974562 | 0.0973276  | 0.0973919  |      3031 |
| macro avg    |   0.401785  | 0.292394   | 0.113368   |      3031 |
| weighted avg |   0.760489  | 0.0973276  | 0.0604673  |      3031 |
### Germeval2019_task3 Dataset
#### Task Offensive labels
|              |   precision |    recall |   f1-score |    support |
|:-------------|------------:|----------:|-----------:|-----------:|
| EXPLICIT     |    0.857143 | 0.994975  |  0.92093   | 796        |
| IMPLICIT     |    0.333333 | 0.0149254 |  0.0285714 | 134        |
| accuracy     |    0.853763 | 0.853763  |  0.853763  |   0.853763 |
| macro avg    |    0.595238 | 0.50495   |  0.474751  | 930        |
| weighted avg |    0.781669 | 0.853763  |  0.792354  | 930        |
### Germeval2021 Dataset
#### Task Toxic
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| 0            |    0.835821 | 0.188552 |   0.307692 | 594        |
| 1            |    0.404938 | 0.937143 |   0.565517 | 350        |
| accuracy     |    0.466102 | 0.466102 |   0.466102 |   0.466102 |
| macro avg    |    0.62038  | 0.562848 |   0.436605 | 944        |
| weighted avg |    0.676066 | 0.466102 |   0.403284 | 944        |
#### Task Engaging
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| 0            |    0.883333 | 0.153401 |   0.261406 | 691        |
| 1            |    0.290049 | 0.944664 |   0.443825 | 253        |
| accuracy     |    0.365466 | 0.365466 |   0.365466 |   0.365466 |
| macro avg    |    0.586691 | 0.549032 |   0.352616 | 944        |
| weighted avg |    0.724328 | 0.365466 |   0.310296 | 944        |
#### Task Factclaiming
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| 0            |    0.837607 | 0.155556 |   0.262383 | 630        |
| 1            |    0.356711 | 0.93949  |   0.51709  | 314        |
| accuracy     |    0.416314 | 0.416314 |   0.416314 |   0.416314 |
| macro avg    |    0.597159 | 0.547523 |   0.389737 | 944        |
| weighted avg |    0.677648 | 0.416314 |   0.347105 | 944        |
# Fine-tuning
## Llama-3.2-3B
### Covid19 Dataset
#### Task Informativeness
|                     |   precision |   recall |   f1-score |    support |
|:--------------------|------------:|---------:|-----------:|-----------:|
| Informative         |    0.922222 | 0.954023 |   0.937853 |  87        |
| Personal_Experience |    0.333333 | 0.571429 |   0.421053 |   7        |
| none                |    0.851852 | 0.657143 |   0.741935 |  35        |
| accuracy            |    0.852713 | 0.852713 |   0.852713 |   0.852713 |
| macro avg           |    0.702469 | 0.727531 |   0.70028  | 129        |
| weighted avg        |    0.871174 | 0.852713 |   0.856654 | 129        |
#### Task Topic
|                   |   precision |   recall |   f1-score |    support |
|:------------------|------------:|---------:|-----------:|-----------:|
| Case_Report       |    0.871795 | 0.918919 |   0.894737 |  37        |
| Consequences      |    0.666667 | 0.333333 |   0.444444 |   6        |
| Governm_Decisions |    0.764706 | 0.590909 |   0.666667 |  22        |
| Risk_Reduction    |    0.666667 | 0.666667 |   0.666667 |   3        |
| Vaccination       |    0.666667 | 0.736842 |   0.7      |  19        |
| none              |    0.630435 | 0.690476 |   0.659091 |  42        |
| accuracy          |    0.728682 | 0.728682 |   0.728682 |   0.728682 |
| macro avg         |    0.711156 | 0.656191 |   0.671934 | 129        |
| weighted avg      |    0.730425 | 0.728682 |   0.724189 | 129        |
#### Task Credibility
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| credible     |    0.84058  | 0.74359  |   0.789116 |  78        |
| non-credible |    0        | 0        |   0        |   3        |
| none         |    0.65     | 0.8125   |   0.722222 |  48        |
| accuracy     |    0.751938 | 0.751938 |   0.751938 |   0.751938 |
| macro avg    |    0.49686  | 0.518697 |   0.503779 | 129        |
| weighted avg |    0.750118 | 0.751938 |   0.745874 | 129        |
### Speech_acts_coarse Dataset
#### Task Coarse labels
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| ASSERTIVE    |    0.720588 | 0.715328 |   0.717949 | 137        |
| COMMISSIVE   |    0.4      | 0.5      |   0.444444 |   4        |
| DIRECTIVE    |    0.939655 | 0.858268 |   0.897119 | 127        |
| EXPRESSIVE   |    0.661972 | 0.5875   |   0.622517 |  80        |
| OTHER        |    0.866667 | 0.928571 |   0.896552 |  14        |
| UNSURE       |    0.265306 | 0.433333 |   0.329114 |  30        |
| accuracy     |    0.719388 | 0.719388 |   0.719388 |   0.719388 |
| macro avg    |    0.642365 | 0.6705   |   0.651282 | 392        |
| weighted avg |    0.746702 | 0.719388 |   0.73035  | 392        |
### Speech_acts_fine Dataset
#### Task Fine labels
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| ADDRESS      |    0.986667 | 0.986667 |   0.986667 |  75        |
| AGREE        |    0        | 0        |   0        |   3        |
| ASSERT       |    0.614583 | 0.5      |   0.551402 | 118        |
| COMMISSIVE   |    0.25     | 0.75     |   0.375    |   4        |
| COMPLAIN     |    0.42     | 0.411765 |   0.415842 |  51        |
| EXCLUDED     |    0        | 0        |   0        |   3        |
| GUESS        |    0.272727 | 0.6      |   0.375    |   5        |
| OTHER        |    0.75     | 0.8      |   0.774194 |  15        |
| PREDICT      |    0        | 0        |   0        |   7        |
| REJOICE      |    0        | 0        |   0        |   3        |
| REQUEST      |    0.851852 | 0.69697  |   0.766667 |  33        |
| REQUIRE      |    0.285714 | 0.375    |   0.324324 |  16        |
| SUGGEST      |    0        | 0        |   0        |   3        |
| SUSTAIN      |    0        | 0        |   0        |   3        |
| UNSURE       |    0.160714 | 0.3      |   0.209302 |  30        |
| WISH         |    0        | 0        |   0        |   2        |
| expressEMOJI |    0.954545 | 1        |   0.976744 |  21        |
| accuracy     |    0.589286 | 0.589286 |   0.589286 |   0.589286 |
| macro avg    |    0.326283 | 0.377671 |   0.338538 | 392        |
| weighted avg |    0.609959 | 0.589286 |   0.593217 | 392        |
### Hasoc2020 Dataset
#### Task Coarse labels
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| HOF          |    0.60452  | 0.798507 |   0.688103 | 134        |
| NOT          |    0.922636 | 0.821429 |   0.869096 | 392        |
| accuracy     |    0.815589 | 0.815589 |   0.815589 |   0.815589 |
| macro avg    |    0.763578 | 0.809968 |   0.778599 | 526        |
| weighted avg |    0.841595 | 0.815589 |   0.822987 | 526        |
#### Task Fine labels
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| HATE         |    0.164706 | 0.583333 |   0.256881 |  24        |
| NONE         |    0.934932 | 0.722222 |   0.814925 | 378        |
| OFFN         |    0.272727 | 0.25     |   0.26087  |  36        |
| PRFN         |    0.594828 | 0.784091 |   0.676471 |  88        |
| accuracy     |    0.693916 | 0.693916 |   0.693916 |   0.693916 |
| macro avg    |    0.491798 | 0.584912 |   0.502287 | 526        |
| weighted avg |    0.797567 | 0.693916 |   0.72838  | 526        |
### Germeval2019_task12 Dataset
#### Task Coarse labels
|              |   precision |   recall |   f1-score |     support |
|:-------------|------------:|---------:|-----------:|------------:|
| OFFENSE      |    0.66634  | 0.7      |   0.682755 |  970        |
| OTHER        |    0.855368 | 0.835032 |   0.845077 | 2061        |
| accuracy     |    0.791818 | 0.791818 |   0.791818 |    0.791818 |
| macro avg    |    0.760854 | 0.767516 |   0.763916 | 3031        |
| weighted avg |    0.794874 | 0.791818 |   0.79313  | 3031        |
#### Task Fine labels
|              |   precision |    recall |   f1-score |     support |
|:-------------|------------:|----------:|-----------:|------------:|
| ABUSE        |    0.363475 | 0.5125    |   0.425311 |  400        |
| INSULT       |    0.304071 | 0.520697  |   0.383936 |  459        |
| OTHER        |    0.880347 | 0.688986  |   0.772999 | 2061        |
| PROFANITY    |    0.147059 | 0.0900901 |   0.111732 |  111        |
| accuracy     |    0.618278 | 0.618278  |   0.618278 |    0.618278 |
| macro avg    |    0.423738 | 0.453068  |   0.423495 | 3031        |
| weighted avg |    0.698013 | 0.618278  |   0.643981 | 3031        |
### Germeval2019_task3 Dataset
#### Task Offensive labels
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| EXPLICIT     |    0.94051  | 0.834171 |   0.884154 | 796        |
| IMPLICIT     |    0.410714 | 0.686567 |   0.513966 | 134        |
| accuracy     |    0.812903 | 0.812903 |   0.812903 |   0.812903 |
| macro avg    |    0.675612 | 0.760369 |   0.69906  | 930        |
| weighted avg |    0.864174 | 0.812903 |   0.830816 | 930        |
### Germeval2021 Dataset
#### Task Toxic
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| 0            |    0.767075 | 0.737374 |   0.751931 | 594        |
| 1            |    0.581769 | 0.62     |   0.600277 | 350        |
| accuracy     |    0.693856 | 0.693856 |   0.693856 |   0.693856 |
| macro avg    |    0.674422 | 0.678687 |   0.676104 | 944        |
| weighted avg |    0.698371 | 0.693856 |   0.695703 | 944        |
#### Task Engaging
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| 0            |    0.841346 | 0.759768 |   0.798479 | 691       |
| 1            |    0.48125  | 0.608696 |   0.537522 | 253       |
| accuracy     |    0.71928  | 0.71928  |   0.71928  |   0.71928 |
| macro avg    |    0.661298 | 0.684232 |   0.668    | 944       |
| weighted avg |    0.744837 | 0.71928  |   0.72854  | 944       |
#### Task Factclaiming
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| 0            |    0.84326  | 0.853968 |   0.84858  | 630       |
| 1            |    0.699346 | 0.681529 |   0.690323 | 314       |
| accuracy     |    0.79661  | 0.79661  |   0.79661  |   0.79661 |
| macro avg    |    0.771303 | 0.767748 |   0.769452 | 944       |
| weighted avg |    0.795391 | 0.79661  |   0.79594  | 944       |
## EuroLLM-9B
### Covid19 Dataset
#### Task Informativeness
|                     |   precision |   recall |   f1-score |    support |
|:--------------------|------------:|---------:|-----------:|-----------:|
| Informative         |    0.91954  | 0.91954  |   0.91954  |  87        |
| Personal_Experience |    0.625    | 0.714286 |   0.666667 |   7        |
| none                |    0.764706 | 0.742857 |   0.753623 |  35        |
| accuracy            |    0.860465 | 0.860465 |   0.860465 |   0.860465 |
| macro avg           |    0.769749 | 0.792228 |   0.779943 | 129        |
| weighted avg        |    0.861548 | 0.860465 |   0.860802 | 129        |
#### Task Topic
|                   |   precision |   recall |   f1-score |    support |
|:------------------|------------:|---------:|-----------:|-----------:|
| Case_Report       |    0.75     | 0.891892 |   0.814815 |  37        |
| Consequences      |    0        | 0        |   0        |   6        |
| Governm_Decisions |    0.833333 | 0.454545 |   0.588235 |  22        |
| Risk_Reduction    |    0.333333 | 0.666667 |   0.444444 |   3        |
| Vaccination       |    0.533333 | 0.421053 |   0.470588 |  19        |
| none              |    0.64     | 0.761905 |   0.695652 |  42        |
| accuracy          |    0.658915 | 0.658915 |   0.658915 |   0.658915 |
| macro avg         |    0.515    | 0.532677 |   0.502289 | 129        |
| weighted avg      |    0.651912 | 0.658915 |   0.640165 | 129        |
#### Task Credibility
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| credible     |    0.951613 | 0.75641  |   0.842857 |  78        |
| non-credible |    0        | 0        |   0        |   3        |
| none         |    0.671642 | 0.9375   |   0.782609 |  48        |
| accuracy     |    0.806202 | 0.806202 |   0.806202 |   0.806202 |
| macro avg    |    0.541085 | 0.564637 |   0.541822 | 129        |
| weighted avg |    0.825307 | 0.806202 |   0.800838 | 129        |
### Speech_acts_coarse Dataset
#### Task Coarse labels
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| ASSERTIVE    |    0.630872 | 0.686131 |   0.657343 | 137        |
| COMMISSIVE   |    1        | 0.25     |   0.4      |   4        |
| DIRECTIVE    |    0.944444 | 0.80315  |   0.868085 | 127        |
| EXPRESSIVE   |    0.550562 | 0.6125   |   0.579882 |  80        |
| OTHER        |    0.857143 | 0.857143 |   0.857143 |  14        |
| UNSURE       |    0.16129  | 0.166667 |   0.163934 |  30        |
| accuracy     |    0.670918 | 0.670918 |   0.670918 |   0.670918 |
| macro avg    |    0.690719 | 0.562598 |   0.587731 | 392        |
| weighted avg |    0.691984 | 0.670918 |   0.676559 | 392        |
### Speech_acts_fine Dataset
#### Task Fine labels
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| ADDRESS      |   0.986667  | 0.986667 |   0.986667 |  75        |
| AGREE        |   0         | 0        |   0        |   3        |
| ASSERT       |   0.592105  | 0.381356 |   0.463918 | 118        |
| COMMISSIVE   |   0.333333  | 0.25     |   0.285714 |   4        |
| COMPLAIN     |   0.314286  | 0.215686 |   0.255814 |  51        |
| EXCLUDED     |   1         | 0.333333 |   0.5      |   3        |
| GUESS        |   0         | 0        |   0        |   5        |
| OTHER        |   0.8125    | 0.866667 |   0.83871  |  15        |
| PREDICT      |   0.0740741 | 0.285714 |   0.117647 |   7        |
| REJOICE      |   0         | 0        |   0        |   3        |
| REQUEST      |   0.380952  | 0.727273 |   0.5      |  33        |
| REQUIRE      |   0.117647  | 0.125    |   0.121212 |  16        |
| SUGGEST      |   0         | 0        |   0        |   3        |
| SUSTAIN      |   0         | 0        |   0        |   3        |
| UNSURE       |   0.204545  | 0.3      |   0.243243 |  30        |
| WISH         |   0         | 0        |   0        |   2        |
| expressEMOJI |   0.954545  | 1        |   0.976744 |  21        |
| accuracy     |   0.517857  | 0.517857 |   0.517857 |   0.517857 |
| macro avg    |   0.33945   | 0.321864 |   0.311157 | 392        |
| weighted avg |   0.55503   | 0.517857 |   0.520623 | 392        |
### Hasoc2020 Dataset
#### Task Coarse labels
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| HOF          |    0.577778 | 0.776119 |   0.66242  | 134        |
| NOT          |    0.913295 | 0.806122 |   0.856369 | 392        |
| accuracy     |    0.798479 | 0.798479 |   0.798479 |   0.798479 |
| macro avg    |    0.745536 | 0.791121 |   0.759394 | 526        |
| weighted avg |    0.827821 | 0.798479 |   0.80696  | 526        |
#### Task Fine labels
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| HATE         |    0.212766 | 0.416667 |   0.28169  |  24        |
| NONE         |    0.936909 | 0.785714 |   0.854676 | 378        |
| OFFN         |    0.236364 | 0.361111 |   0.285714 |  36        |
| PRFN         |    0.551402 | 0.670455 |   0.605128 |  88        |
| accuracy     |    0.720532 | 0.720532 |   0.720532 |   0.720532 |
| macro avg    |    0.48436  | 0.558487 |   0.506802 | 526        |
| weighted avg |    0.791426 | 0.720532 |   0.747843 | 526        |
### Germeval2019_task12 Dataset
#### Task Coarse labels
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| OFFENSE      |    0.636442 | 0.759794 |   0.692669 |  970       |
| OTHER        |    0.875601 | 0.79573  |   0.833757 | 2061       |
| accuracy     |    0.78423  | 0.78423  |   0.78423  |    0.78423 |
| macro avg    |    0.756021 | 0.777762 |   0.763213 | 3031       |
| weighted avg |    0.799064 | 0.78423  |   0.788605 | 3031       |
#### Task Fine labels
|              |   precision |   recall |   f1-score |     support |
|:-------------|------------:|---------:|-----------:|------------:|
| ABUSE        |    0.279391 | 0.505    |   0.359751 |  400        |
| INSULT       |    0.310112 | 0.300654 |   0.30531  |  459        |
| OTHER        |    0.859259 | 0.6754   |   0.756316 | 2061        |
| PROFANITY    |    0.139918 | 0.306306 |   0.19209  |  111        |
| accuracy     |    0.582646 | 0.582646 |   0.582646 |    0.582646 |
| macro avg    |    0.39717  | 0.44684  |   0.403367 | 3031        |
| weighted avg |    0.673231 | 0.582646 |   0.615021 | 3031        |
### Germeval2019_task3 Dataset
#### Task Offensive labels
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| EXPLICIT     |    0.917313 | 0.89196  |   0.904459 | 796       |
| IMPLICIT     |    0.448718 | 0.522388 |   0.482759 | 134       |
| accuracy     |    0.83871  | 0.83871  |   0.83871  |   0.83871 |
| macro avg    |    0.683015 | 0.707174 |   0.693609 | 930       |
| weighted avg |    0.849795 | 0.83871  |   0.843698 | 930       |
### Germeval2021 Dataset
#### Task Toxic
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| 0            |    0.761146 | 0.804714 |   0.782324 | 594       |
| 1            |    0.632911 | 0.571429 |   0.600601 | 350       |
| accuracy     |    0.71822  | 0.71822  |   0.71822  |   0.71822 |
| macro avg    |    0.697029 | 0.688071 |   0.691462 | 944       |
| weighted avg |    0.713602 | 0.71822  |   0.714948 | 944       |
#### Task Engaging
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| 0            |    0.834365 | 0.780029 |   0.806283 | 691        |
| 1            |    0.489933 | 0.577075 |   0.529946 | 253        |
| accuracy     |    0.725636 | 0.725636 |   0.725636 |   0.725636 |
| macro avg    |    0.662149 | 0.678552 |   0.668114 | 944        |
| weighted avg |    0.742055 | 0.725636 |   0.732222 | 944        |
#### Task Factclaiming
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| 0            |    0.847039 | 0.81746  |   0.831987 | 630        |
| 1            |    0.657738 | 0.703822 |   0.68     | 314        |
| accuracy     |    0.779661 | 0.779661 |   0.779661 |   0.779661 |
| macro avg    |    0.752389 | 0.760641 |   0.755994 | 944        |
| weighted avg |    0.784073 | 0.779661 |   0.781432 | 944        |
## Teuken-7B
### Covid19 Dataset
#### Task Informativeness
|                     |   precision |   recall |   f1-score |   support |
|:--------------------|------------:|---------:|-----------:|----------:|
| Informative         |    0.879121 | 0.91954  |   0.898876 |  87       |
| Personal_Experience |    0.333333 | 0.571429 |   0.421053 |   7       |
| none                |    0.730769 | 0.542857 |   0.622951 |  35       |
| accuracy            |    0.79845  | 0.79845  |   0.79845  |   0.79845 |
| macro avg           |    0.647741 | 0.677942 |   0.647627 | 129       |
| weighted avg        |    0.809254 | 0.79845  |   0.798084 | 129       |
#### Task Topic
|                   |   precision |   recall |   f1-score |    support |
|:------------------|------------:|---------:|-----------:|-----------:|
| Case_Report       |    0.666667 | 0.756757 |   0.708861 |  37        |
| Consequences      |    0.4      | 0.333333 |   0.363636 |   6        |
| Governm_Decisions |    0.615385 | 0.727273 |   0.666667 |  22        |
| Risk_Reduction    |    0.5      | 1        |   0.666667 |   3        |
| Vaccination       |    0.466667 | 0.368421 |   0.411765 |  19        |
| none              |    0.6      | 0.5      |   0.545455 |  42        |
| accuracy          |    0.596899 | 0.596899 |   0.596899 |   0.596899 |
| macro avg         |    0.541453 | 0.614297 |   0.560508 | 129        |
| weighted avg      |    0.590479 | 0.596899 |   0.587666 | 129        |
#### Task Credibility
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| credible     |    0.828125 | 0.679487 |   0.746479 |  78        |
| non-credible |    0        | 0        |   0        |   3        |
| none         |    0.578125 | 0.770833 |   0.660714 |  48        |
| accuracy     |    0.697674 | 0.697674 |   0.697674 |   0.697674 |
| macro avg    |    0.46875  | 0.48344  |   0.469064 | 129        |
| weighted avg |    0.715843 | 0.697674 |   0.697206 | 129        |
### Speech_acts_coarse Dataset
#### Task Coarse labels
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| ASSERTIVE    |    0.716049 | 0.423358 |   0.53211  | 137        |
| COMMISSIVE   |    0.666667 | 0.5      |   0.571429 |   4        |
| DIRECTIVE    |    0.866667 | 0.716535 |   0.784483 | 127        |
| EXPRESSIVE   |    0.494737 | 0.5875   |   0.537143 |  80        |
| OTHER        |    0.52381  | 0.785714 |   0.628571 |  14        |
| UNSURE       |    0.206897 | 0.6      |   0.307692 |  30        |
| accuracy     |    0.579082 | 0.579082 |   0.579082 |   0.579082 |
| macro avg    |    0.579138 | 0.602185 |   0.560238 | 392        |
| weighted avg |    0.673345 | 0.579082 |   0.601572 | 392        |
### Speech_acts_fine Dataset
#### Task Fine labels
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| ADDRESS      |    1        | 0.973333 |   0.986486 |  75        |
| AGREE        |    0.5      | 0.333333 |   0.4      |   3        |
| ASSERT       |    0.62963  | 0.288136 |   0.395349 | 118        |
| COMMISSIVE   |    0.375    | 0.75     |   0.5      |   4        |
| COMPLAIN     |    0.375    | 0.352941 |   0.363636 |  51        |
| EXCLUDED     |    0.2      | 0.333333 |   0.25     |   3        |
| GUESS        |    0.333333 | 0.2      |   0.25     |   5        |
| OTHER        |    0.733333 | 0.733333 |   0.733333 |  15        |
| PREDICT      |    0.1875   | 0.428571 |   0.26087  |   7        |
| REJOICE      |    0        | 0        |   0        |   3        |
| REQUEST      |    0.590909 | 0.787879 |   0.675325 |  33        |
| REQUIRE      |    0.571429 | 0.25     |   0.347826 |  16        |
| SUGGEST      |    0.166667 | 0.333333 |   0.222222 |   3        |
| SUSTAIN      |    0        | 0        |   0        |   3        |
| UNSURE       |    0.175    | 0.466667 |   0.254545 |  30        |
| WISH         |    0        | 0        |   0        |   2        |
| expressEMOJI |    0.954545 | 1        |   0.976744 |  21        |
| accuracy     |    0.538265 | 0.538265 |   0.538265 |   0.538265 |
| macro avg    |    0.39955  | 0.425345 |   0.389196 | 392        |
| weighted avg |    0.613364 | 0.538265 |   0.545599 | 392        |
### Hasoc2020 Dataset
#### Task Coarse labels
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| HOF          |    0.643312 | 0.753731 |   0.694158 | 134        |
| NOT          |    0.910569 | 0.857143 |   0.883049 | 392        |
| accuracy     |    0.830798 | 0.830798 |   0.830798 |   0.830798 |
| macro avg    |    0.776941 | 0.805437 |   0.788603 | 526        |
| weighted avg |    0.842485 | 0.830798 |   0.834928 | 526        |
#### Task Fine labels
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| HATE         |    0.212121 | 0.583333 |   0.311111 |  24        |
| NONE         |    0.925094 | 0.653439 |   0.765891 | 378        |
| OFFN         |    0.220779 | 0.472222 |   0.300885 |  36        |
| PRFN         |    0.491379 | 0.647727 |   0.558824 |  88        |
| accuracy     |    0.636882 | 0.636882 |   0.636882 |   0.636882 |
| macro avg    |    0.462343 | 0.58918  |   0.484178 | 526        |
| weighted avg |    0.771798 | 0.636882 |   0.678673 | 526        |
### Germeval2019_task12 Dataset
#### Task Coarse labels
|              |   precision |   recall |   f1-score |     support |
|:-------------|------------:|---------:|-----------:|------------:|
| OFFENSE      |    0.598023 | 0.748454 |   0.664835 |  970        |
| OTHER        |    0.865713 | 0.763222 |   0.811243 | 2061        |
| accuracy     |    0.758496 | 0.758496 |   0.758496 |    0.758496 |
| macro avg    |    0.731868 | 0.755838 |   0.738039 | 3031        |
| weighted avg |    0.780045 | 0.758496 |   0.764389 | 3031        |
#### Task Fine labels
|              |   precision |   recall |   f1-score |     support |
|:-------------|------------:|---------:|-----------:|------------:|
| ABUSE        |    0.337438 | 0.685    |   0.452145 |  400        |
| INSULT       |    0.349333 | 0.285403 |   0.314149 |  459        |
| OTHER        |    0.874489 | 0.726832 |   0.793853 | 2061        |
| PROFANITY    |    0.183206 | 0.216216 |   0.198347 |  111        |
| accuracy     |    0.635764 | 0.635764 |   0.635764 |    0.635764 |
| macro avg    |    0.436117 | 0.478363 |   0.439623 | 3031        |
| weighted avg |    0.698772 | 0.635764 |   0.654305 | 3031        |
### Germeval2019_task3 Dataset
#### Task Offensive labels
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| EXPLICIT     |    0.931129 | 0.849246 |   0.888305 | 796        |
| IMPLICIT     |    0.411765 | 0.626866 |   0.497041 | 134        |
| accuracy     |    0.817204 | 0.817204 |   0.817204 |   0.817204 |
| macro avg    |    0.671447 | 0.738056 |   0.692673 | 930        |
| weighted avg |    0.856296 | 0.817204 |   0.831929 | 930        |
### Germeval2021 Dataset
#### Task Toxic
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| 0            |    0.785467 | 0.76431  |   0.774744 | 594        |
| 1            |    0.617486 | 0.645714 |   0.631285 | 350        |
| accuracy     |    0.720339 | 0.720339 |   0.720339 |   0.720339 |
| macro avg    |    0.701477 | 0.705012 |   0.703014 | 944        |
| weighted avg |    0.723186 | 0.720339 |   0.721555 | 944        |
#### Task Engaging
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| 0            |    0.827922 | 0.738061 |   0.780413 | 691        |
| 1            |    0.448171 | 0.581028 |   0.506024 | 253        |
| accuracy     |    0.695975 | 0.695975 |   0.695975 |   0.695975 |
| macro avg    |    0.638046 | 0.659544 |   0.643219 | 944        |
| weighted avg |    0.726145 | 0.695975 |   0.706875 | 944        |
#### Task Factclaiming
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| 0            |    0.816294 | 0.811111 |   0.813694 | 630        |
| 1            |    0.625786 | 0.633758 |   0.629747 | 314        |
| accuracy     |    0.752119 | 0.752119 |   0.752119 |   0.752119 |
| macro avg    |    0.72104  | 0.722435 |   0.721721 | 944        |
| weighted avg |    0.752926 | 0.752119 |   0.752508 | 944        |
## BübleLM
### Covid19 Dataset
#### Task Informativeness
|                     |   precision |   recall |   f1-score |    support |
|:--------------------|------------:|---------:|-----------:|-----------:|
| Informative         |    0.891304 | 0.942529 |   0.916201 |  87        |
| Personal_Experience |    0.5      | 0.571429 |   0.533333 |   7        |
| none                |    0.793103 | 0.657143 |   0.71875  |  35        |
| accuracy            |    0.844961 | 0.844961 |   0.844961 |   0.844961 |
| macro avg           |    0.728136 | 0.7237   |   0.722761 | 129        |
| weighted avg        |    0.843427 | 0.844961 |   0.841853 | 129        |
#### Task Topic
|                   |   precision |   recall |   f1-score |    support |
|:------------------|------------:|---------:|-----------:|-----------:|
| Case_Report       |    0.733333 | 0.891892 |   0.804878 |  37        |
| Consequences      |    0.4      | 0.333333 |   0.363636 |   6        |
| Governm_Decisions |    0.5      | 0.545455 |   0.521739 |  22        |
| Risk_Reduction    |    1        | 0.666667 |   0.8      |   3        |
| Vaccination       |    0.444444 | 0.631579 |   0.521739 |  19        |
| none              |    0.576923 | 0.357143 |   0.441176 |  42        |
| accuracy          |    0.589147 | 0.589147 |   0.589147 |   0.589147 |
| macro avg         |    0.609117 | 0.571011 |   0.575528 | 129        |
| weighted avg      |    0.590764 | 0.589147 |   0.575837 | 129        |
#### Task Credibility
|              |   precision |   recall |   f1-score |   support |
|:-------------|------------:|---------:|-----------:|----------:|
| credible     |    0.859155 | 0.782051 |   0.818792 |  78       |
| non-credible |    0        | 0        |   0        |   3       |
| none         |    0.660714 | 0.770833 |   0.711538 |  48       |
| accuracy     |    0.75969  | 0.75969  |   0.75969  |   0.75969 |
| macro avg    |    0.506623 | 0.517628 |   0.51011  | 129       |
| weighted avg |    0.765336 | 0.75969  |   0.759842 | 129       |
### Speech_acts_coarse Dataset
#### Task Coarse labels
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| ASSERTIVE    |    0.631579 | 0.613139 |   0.622222 | 137        |
| COMMISSIVE   |    0.133333 | 0.5      |   0.210526 |   4        |
| DIRECTIVE    |    0.90099  | 0.716535 |   0.798246 | 127        |
| EXPRESSIVE   |    0.49     | 0.6125   |   0.544444 |  80        |
| OTHER        |    0.5      | 0.571429 |   0.533333 |  14        |
| UNSURE       |    0.222222 | 0.2      |   0.210526 |  30        |
| accuracy     |    0.612245 | 0.612245 |   0.612245 |   0.612245 |
| macro avg    |    0.479687 | 0.5356   |   0.48655  | 392        |
| weighted avg |    0.648857 | 0.612245 |   0.624494 | 392        |
### Speech_acts_fine Dataset
#### Task Fine labels
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| ADDRESS      |    0.931507 | 0.906667 |   0.918919 |  75        |
| AGREE        |    0.5      | 0.666667 |   0.571429 |   3        |
| ASSERT       |    0.486111 | 0.29661  |   0.368421 | 118        |
| COMMISSIVE   |    0.181818 | 0.5      |   0.266667 |   4        |
| COMPLAIN     |    0.25     | 0.215686 |   0.231579 |  51        |
| EXCLUDED     |    0        | 0        |   0        |   3        |
| GUESS        |    0        | 0        |   0        |   5        |
| OTHER        |    0.470588 | 0.533333 |   0.5      |  15        |
| PREDICT      |    0        | 0        |   0        |   7        |
| REJOICE      |    0        | 0        |   0        |   3        |
| REQUEST      |    0.525    | 0.636364 |   0.575342 |  33        |
| REQUIRE      |    0.2      | 0.125    |   0.153846 |  16        |
| SUGGEST      |    0        | 0        |   0        |   3        |
| SUSTAIN      |    0        | 0        |   0        |   3        |
| UNSURE       |    0.173333 | 0.433333 |   0.247619 |  30        |
| WISH         |    0        | 0        |   0        |   2        |
| expressEMOJI |    0.954545 | 1        |   0.976744 |  21        |
| accuracy     |    0.466837 | 0.466837 |   0.466837 |   0.466837 |
| macro avg    |    0.274877 | 0.312568 |   0.282974 | 392        |
| weighted avg |    0.497527 | 0.466837 |   0.469062 | 392        |
### Hasoc2020 Dataset
#### Task Coarse labels
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| HOF          |    0.679104 | 0.679104 |   0.679104 | 134        |
| NOT          |    0.890306 | 0.890306 |   0.890306 | 392        |
| accuracy     |    0.836502 | 0.836502 |   0.836502 |   0.836502 |
| macro avg    |    0.784705 | 0.784705 |   0.784705 | 526        |
| weighted avg |    0.836502 | 0.836502 |   0.836502 | 526        |
#### Task Fine labels
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| HATE         |    0.222222 | 0.333333 |   0.266667 |  24        |
| NONE         |    0.892537 | 0.791005 |   0.83871  | 378        |
| OFFN         |    0.209677 | 0.361111 |   0.265306 |  36        |
| PRFN         |    0.634409 | 0.670455 |   0.651934 |  88        |
| accuracy     |    0.720532 | 0.720532 |   0.720532 |   0.720532 |
| macro avg    |    0.489711 | 0.538976 |   0.505654 | 526        |
| weighted avg |    0.772032 | 0.720532 |   0.742117 | 526        |
### Germeval2019_task12 Dataset
#### Task Coarse labels
|              |   precision |   recall |   f1-score |     support |
|:-------------|------------:|---------:|-----------:|------------:|
| OFFENSE      |    0.585943 | 0.790722 |   0.673102 |  970        |
| OTHER        |    0.882114 | 0.737021 |   0.803066 | 2061        |
| accuracy     |    0.754207 | 0.754207 |   0.754207 |    0.754207 |
| macro avg    |    0.734029 | 0.763871 |   0.738084 | 3031        |
| weighted avg |    0.787331 | 0.754207 |   0.761474 | 3031        |
#### Task Fine labels
|              |   precision |   recall |   f1-score |     support |
|:-------------|------------:|---------:|-----------:|------------:|
| ABUSE        |    0.312602 | 0.4775   |   0.377844 |  400        |
| INSULT       |    0.332335 | 0.48366  |   0.393966 |  459        |
| OTHER        |    0.862534 | 0.621058 |   0.722144 | 2061        |
| PROFANITY    |    0.11194  | 0.27027  |   0.158311 |  111        |
| accuracy     |    0.568459 | 0.568459 |   0.568459 |    0.568459 |
| macro avg    |    0.404853 | 0.463122 |   0.413066 | 3031        |
| weighted avg |    0.682181 | 0.568459 |   0.606361 | 3031        |
### Germeval2019_task3 Dataset
#### Task Offensive labels
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| EXPLICIT     |    0.932665 | 0.817839 |   0.871486 | 796        |
| IMPLICIT     |    0.375    | 0.649254 |   0.47541  | 134        |
| accuracy     |    0.793548 | 0.793548 |   0.793548 |   0.793548 |
| macro avg    |    0.653832 | 0.733546 |   0.673448 | 930        |
| weighted avg |    0.852313 | 0.793548 |   0.814417 | 930        |
### Germeval2021 Dataset
#### Task Toxic
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| 0            |    0.74     | 0.809764 |   0.773312 | 594        |
| 1            |    0.615646 | 0.517143 |   0.562112 | 350        |
| accuracy     |    0.701271 | 0.701271 |   0.701271 |   0.701271 |
| macro avg    |    0.677823 | 0.663454 |   0.667712 | 944        |
| weighted avg |    0.693894 | 0.701271 |   0.695007 | 944        |
#### Task Engaging
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| 0            |    0.83308  | 0.794501 |   0.813333 | 691        |
| 1            |    0.501754 | 0.565217 |   0.531599 | 253        |
| accuracy     |    0.733051 | 0.733051 |   0.733051 |   0.733051 |
| macro avg    |    0.667417 | 0.679859 |   0.672466 | 944        |
| weighted avg |    0.744282 | 0.733051 |   0.737826 | 944        |
#### Task Factclaiming
|              |   precision |   recall |   f1-score |    support |
|:-------------|------------:|---------:|-----------:|-----------:|
| 0            |    0.808176 | 0.815873 |   0.812006 | 630        |
| 1            |    0.623377 | 0.611465 |   0.617363 | 314        |
| accuracy     |    0.747881 | 0.747881 |   0.747881 |   0.747881 |
| macro avg    |    0.715776 | 0.713669 |   0.714685 | 944        |
| weighted avg |    0.746707 | 0.747881 |   0.747263 | 944        |